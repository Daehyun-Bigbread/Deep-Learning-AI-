{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oxford Pet 데이터 세트를 Tensorflow Object Detection API를 이용하여 학습 및 Object Detection 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow object detection 설치하기\n",
    "* tensorflow object detection github를 복사. git clone https://github.com/tensorflow/models.git\n",
    "\n",
    "* tensorflow object detection API은 models/research/object_detection 내에 주요 API가 있지만 models/research의 다른 패키지도 사용\n",
    "\n",
    "* models/research 에 setup.py 를 수행하기 전에 미리 protocol buffer 구동을 위한 proto compiler 설정과 protocol buffer 형태의 파일을 python 형태로 변환하는 작업 필요. \n",
    "\n",
    "* protoc 실행 시 command not found가 나오면 아래를 실행하여 install\n",
    "\n",
    "    PROTOC_ZIP=protoc-3.7.1-linux-x86_64.zip   \n",
    "    curl -OL https://github.com/protocolbuffers/protobuf/releases/download/v3.7.1/$PROTOC_ZIP  \n",
    "    \n",
    "    sudo unzip -o $PROTOC_ZIP -d /usr/local bin/protoc\n",
    "\n",
    "    sudo unzip -o $PROTOC_ZIP -d /usr/local 'include/*'\n",
    "\n",
    "    rm -f $PROTOC_ZIP\n",
    "* models/research/ 디렉토리에서 아래 명령어를 실행하여 모든 proto 타입 파일이 .py 로 변경 수행 후 확인  \n",
    "    protoc object_detection/protos/*.proto --python_out=.\n",
    "    \n",
    "* 이후에 models/research 에서 setup.py를 실행. setup.py는 object_detection 디렉토리에 있는 모듈만 셋업. \n",
    "    cd models/research\n",
    "    python setup.py install\n",
    "    \n",
    "* Object Detection API는 models/research/object_detection 뿐만 아니라  models/research의 타 디렉토리에 있는 모듈도 참조함. 이를 위해 models/research 디렉토리를 Path로 설정하고 일부 패키지는 Local file기반으로 import 해야 함. \n",
    "\n",
    "    sys.path.append(..../models/research)\n",
    "    \n",
    "* object_detection/builders/model_builder_test.py 를 수행하여 정상적으로 셋업이 되었는지 확인.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습을 위한 디렉토리 생성하고 Pretrained 모델을 다운로드 받기\n",
    "* wget http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oxford Pet Class name 매핑 파일 pbtxt 다운로드 받기 \n",
    "* wget https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/pet_label_map.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF_RECORD 데이터 세트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export TRAIN_DIR=~/DLCV/Detection/tensor_api/train_pet\n",
    "#export DATA_DIR=~/DLCV/data/ox_pet_tensor\n",
    "#export TENSOR_OBJ_DIR=~/DLCV/Detection/tensor_api/model/research/object_detection\n",
    "#export PYTHONPATH=$PYTHONPATH:~/DLCV/Detection/tensor_api/models/research:~/DLCV/Detection/tensor_api/models/research/object_detection\n",
    "#python $TENSOR_OBJ_DIR/dataset_tools/create_pet_tf_record.py --label_map_path=$TRAIN_DIR/annotations/pet_label_map.pbtxt --data_dir=$DATA_DIR --output_dir=$TRAIN_DIR/annotations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oxford Pet을 학습하기 위한 config 파일 다운로드 받기\n",
    "* wget https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_inception_v2_pets.config\n",
    "* 다운로드 후 pretrained model checkpoint,  tfrecord 데이터 세트, 기타 환경 변수 재 설정. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oxford Pet 데이터 학습 수행. \n",
    "* python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_inception_v2_coco.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습이 완료 된 후 학습 모델을 Inference 모델로 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python export_inference_graph.py --input_type image_tensor --pipeline_config_path=config/ssd_inception_v2_pets.config \\\n",
    "# --trained_checkpoint_prefix=training/model.ckpt-5000 --output_directory=training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 변환된 frozen graph 파일을 이용하여 Object Detection 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "HOME_DIR = str(Path.home())\n",
    "\n",
    "DATA_DIR = os.path.join(HOME_DIR, 'DLCV/data/ox_pet')\n",
    "ANNO_DIR = os.path.join(DATA_DIR, 'annotations')\n",
    "IMAGE_DIR = os.path.join(DATA_DIR,  'images')\n",
    "\n",
    "print(os.listdir(IMAGE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow는 Class id를 1 부터 부여함에 유의 \n",
    "labels_to_names = {1: 'Abyssinian', 2: 'Bengal', 3: 'Birman', 4: 'Bombay', 5: 'British_Shorthair', 6: 'Egyptian_Mau', 7: 'Maine_Coon',\n",
    " 8: 'Persian', 9: 'Ragdoll', 10: 'Russian_Blue', 11: 'Siamese', 12: 'Sphynx', 13: 'american_bulldog', 14: 'american_pit_bull_terrier',\n",
    " 15: 'basset_hound', 16: 'beagle', 17: 'boxer', 18: 'chihuahua', 19: 'english_cocker_spaniel', 20: 'english_setter', 21: 'german_shorthaired',\n",
    " 22: 'great_pyrenees', 23: 'havanese', 24: 'japanese_chin', 25: 'keeshond', 26: 'leonberger', 27: 'miniature_pinscher', 28: 'newfoundland',\n",
    " 29: 'pomeranian', 30: 'pug', 31: 'saint_bernard', 32: 'samoyed', 33: 'scottish_terrier', 34: 'shiba_inu', 35: 'staffordshire_bull_terrier',\n",
    " 36: 'wheaten_terrier', 37: 'yorkshire_terrier'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def get_tensor_detected_image(sess, img_array, use_copied_array):\n",
    "    \n",
    "    rows = img_array.shape[0]\n",
    "    cols = img_array.shape[1]\n",
    "    if use_copied_array:\n",
    "        draw_img = img_array.copy()\n",
    "    else:\n",
    "        draw_img = img_array\n",
    "    \n",
    "    inp = img_array[:, :, [2, 1, 0]]  # BGR2RGB\n",
    "    \n",
    "    start = time.time()\n",
    "    # Object Detection 수행. \n",
    "    out = sess.run([sess.graph.get_tensor_by_name('num_detections:0'),\n",
    "                    sess.graph.get_tensor_by_name('detection_scores:0'),\n",
    "                    sess.graph.get_tensor_by_name('detection_boxes:0'),\n",
    "                    sess.graph.get_tensor_by_name('detection_classes:0')],\n",
    "                   feed_dict={'image_tensor:0': inp.reshape(1, inp.shape[0], inp.shape[1], 3)})\n",
    "    \n",
    "    green_color=(0, 255, 0)\n",
    "    red_color=(0, 0, 255)\n",
    "    \n",
    "    # Bounding Box 시각화 \n",
    "    # Detect된 Object 별로 bounding box 시각화 \n",
    "    num_detections = int(out[0][0])\n",
    "    for i in range(num_detections):\n",
    "        # class id와 object class score, bounding box정보를 추출\n",
    "        classId = int(out[3][0][i])\n",
    "        score = float(out[1][0][i])\n",
    "        bbox = [float(v) for v in out[2][0][i]]\n",
    "        if score > 0.4:\n",
    "            left = bbox[1] * cols\n",
    "            top = bbox[0] * rows\n",
    "            right = bbox[3] * cols\n",
    "            bottom = bbox[2] * rows\n",
    "            # cv2의 rectangle(), putText()로 bounding box의 클래스명 시각화 \n",
    "            cv2.rectangle(draw_img, (int(left), int(top)), (int(right), int(bottom)), green_color, thickness=2)\n",
    "            caption = \"{}: {:.4f}\".format(labels_to_names[classId], score)\n",
    "            cv2.putText(draw_img, caption, (int(left), int(top + 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, red_color, 1)\n",
    "    \n",
    "    print('Detection 수행시간:',round(time.time() - start, 3),\"초\")\n",
    "    return draw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference graph를 읽음. .\n",
    "with tf.gfile.FastGFile('./train_pet/training/frozen_inference_graph.pb', 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    # Session 시작하고 inference graph 모델 로딩 \n",
    "    sess.graph.as_default()\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "    \n",
    "    # 입력 이미지 생성, Object Detection된 image 반환, 반환된 image의 BGR을 RGB로 변경 \n",
    "    for image_file in ['Russian_Blue_29.jpg', 'yorkshire_terrier_176.jpg', 'german_shorthaired_127.jpg' ]:\n",
    "        img = cv2.imread(os.path.join(IMAGE_DIR, image_file))\n",
    "        draw_img = get_tensor_detected_image(sess, img, True)\n",
    "\n",
    "        img_rgb = cv2.cvtColor(draw_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 세트를 TFRecord로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import contextlib2\n",
    "from lxml import etree\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.dataset_tools import tf_record_creation_util\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기준 디렉토리 재 설정."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  pathlib import Path\n",
    "\n",
    "HOME_DIR = str(Path.home())\n",
    "\n",
    "DATA_DIR = os.path.join(HOME_DIR, 'DLCV/data/ox_pet')\n",
    "ANNO_DIR = os.path.join(DATA_DIR, 'annotations')\n",
    "IMAGE_DIR = os.path.join(DATA_DIR,  'images')\n",
    "# pets 데이터세트를 학습을 위한 기본 directory설정. \n",
    "TRAIN_DIR = os.path.join(HOME_DIR, 'DLCV/Detection/tensor_api/train_pet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체 파일, train 파일, valid 파일 추출."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_list = []\n",
    "\n",
    "for xml_file in os.listdir(ANNO_DIR):\n",
    "    index = xml_file.rfind('.xml')\n",
    "    if index > - 1:\n",
    "        xml_file = xml_file[:xml_file.rfind('.xml')]\n",
    "        examples_list.append(xml_file)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(examples_list)\n",
    "\n",
    "num_examples = len(examples_list)\n",
    "num_train = int(0.9 * num_examples)\n",
    "train_examples = examples_list[:num_train]\n",
    "val_examples = examples_list[num_train:]\n",
    "\n",
    "print('example file count:', len(examples_list), 'train file count:',len(train_examples), \\\n",
    "      'val file count count:', len(val_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 개별 xml 파일을 입력 받아서 tf Example로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_name_from_filename(file_name):\n",
    "    file_breed = file_name[0:file_name.rfind('_')]\n",
    "    return file_breed\n",
    "\n",
    "def dict_to_tf_example(data, label_map_dict, image_subdirectory,\n",
    "                       ignore_difficult_instances=False):\n",
    "  \n",
    "    img_path = os.path.join(image_subdirectory, data['filename'])\n",
    "    with tf.gfile.GFile(img_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    \n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = PIL.Image.open(encoded_jpg_io)\n",
    "    if image.format != 'JPEG':\n",
    "        raise ValueError('Image format not JPEG')\n",
    "    \n",
    "    key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "    width = int(data['size']['width'])\n",
    "    height = int(data['size']['height'])\n",
    "\n",
    "    xmins = []\n",
    "    ymins = []\n",
    "    xmaxs = []\n",
    "    ymaxs = []\n",
    "    classes = []\n",
    "    classes_text = []\n",
    "    truncated = []\n",
    "    poses = []\n",
    "    difficult_obj = []\n",
    "    masks = []\n",
    "    # xml\n",
    "    if 'object' in data:\n",
    "        for obj in data['object']:\n",
    "            difficult = bool(int(obj['difficult']))\n",
    "            if ignore_difficult_instances and difficult:\n",
    "                continue\n",
    "            difficult_obj.append(int(difficult))\n",
    "\n",
    "            xmin = float(obj['bndbox']['xmin'])\n",
    "            xmax = float(obj['bndbox']['xmax'])\n",
    "            ymin = float(obj['bndbox']['ymin'])\n",
    "            ymax = float(obj['bndbox']['ymax'])\n",
    "            \n",
    "            xmins.append(xmin / width)\n",
    "            ymins.append(ymin / height)\n",
    "            xmaxs.append(xmax / width)\n",
    "            ymaxs.append(ymax / height)\n",
    "            class_name = get_class_name_from_filename(data['filename'])\n",
    "            classes_text.append(class_name.encode('utf8'))\n",
    "            classes.append(label_map_dict[class_name])\n",
    "            truncated.append(int(obj['truncated']))\n",
    "            poses.append(obj['pose'].encode('utf8'))\n",
    "  \n",
    "    feature_dict = {\n",
    "      'image/height': dataset_util.int64_feature(height),\n",
    "      'image/width': dataset_util.int64_feature(width),\n",
    "      'image/filename': dataset_util.bytes_feature(\n",
    "          data['filename'].encode('utf8')),\n",
    "      'image/source_id': dataset_util.bytes_feature(\n",
    "          data['filename'].encode('utf8')),\n",
    "      'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n",
    "      'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "      'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "      'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "      'image/object/difficult': dataset_util.int64_list_feature(difficult_obj),\n",
    "      'image/object/truncated': dataset_util.int64_list_feature(truncated),\n",
    "      'image/object/view': dataset_util.bytes_list_feature(poses),\n",
    "    }\n",
    "  \n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### output filename과 annotations, images 디렉토리등을 입력받아 TFRecord를 생성하는 함수 생성. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_record(output_filename, num_shards, label_map_dict, annotations_dir,\n",
    "                     image_dir, examples):\n",
    " \n",
    "    with contextlib2.ExitStack() as tf_record_close_stack:\n",
    "        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n",
    "            tf_record_close_stack, output_filename, num_shards)\n",
    "        for idx, example in enumerate(examples):\n",
    "            if idx % 100 == 0:\n",
    "                print('On image {} of {}'.format(idx, len(examples)))\n",
    "            xml_path = os.path.join(annotations_dir, example + '.xml')\n",
    "\n",
    "            if not os.path.exists(xml_path):\n",
    "                logging.warning('Could not find %s, ignoring example.', xml_path)\n",
    "                continue\n",
    "            with tf.gfile.GFile(xml_path, 'r') as fid:\n",
    "                xml_str = fid.read()\n",
    "            xml = etree.fromstring(xml_str)\n",
    "            data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n",
    "\n",
    "            try:\n",
    "                tf_example = dict_to_tf_example(data, label_map_dict, image_dir)\n",
    "                if tf_example:\n",
    "                    shard_idx = idx % num_shards\n",
    "                    output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n",
    "            except ValueError:\n",
    "                print('Invalid example: %s, ignoring.', xml_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train용, validation용 TFRecord 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "\n",
    "label_map_dict = label_map_util.get_label_map_dict(os.path.join(TRAIN_DIR, 'annotations/pet_label_map.pbtxt'))\n",
    "train_output_path = os.path.join(TRAIN_DIR, 'annotations/pet_train_new.record')\n",
    "val_output_path = os.path.join(TRAIN_DIR, 'annotations/pet_val_new.record')\n",
    "\n",
    "\n",
    "create_tf_record(train_output_path,1, label_map_dict, ANNO_DIR, IMAGE_DIR, train_examples)\n",
    "create_tf_record(val_output_path, 1, label_map_dict, ANNO_DIR, IMAGE_DIR, val_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python train.py --logtostderr --train_dir=training/ --pipeline_config_path=config/pets_new.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 학습된 모델을 이용하여 Inferece 모델 생성. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python export_inference_graph.py --input_type image_tensor --pipeline_config_path=config/pets_new.config \\\n",
    "# --trained_checkpoint_prefix=training/model.ckpt-15000 --output_directory=training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow는 Class id를 1 부터 부여함에 유의 \n",
    "labels_to_names = {1: 'Abyssinian', 2: 'Bengal', 3: 'Birman', 4: 'Bombay', 5: 'British_Shorthair', 6: 'Egyptian_Mau', 7: 'Maine_Coon',\n",
    " 8: 'Persian', 9: 'Ragdoll', 10: 'Russian_Blue', 11: 'Siamese', 12: 'Sphynx', 13: 'american_bulldog', 14: 'american_pit_bull_terrier',\n",
    " 15: 'basset_hound', 16: 'beagle', 17: 'boxer', 18: 'chihuahua', 19: 'english_cocker_spaniel', 20: 'english_setter', 21: 'german_shorthaired',\n",
    " 22: 'great_pyrenees', 23: 'havanese', 24: 'japanese_chin', 25: 'keeshond', 26: 'leonberger', 27: 'miniature_pinscher', 28: 'newfoundland',\n",
    " 29: 'pomeranian', 30: 'pug', 31: 'saint_bernard', 32: 'samoyed', 33: 'scottish_terrier', 34: 'shiba_inu', 35: 'staffordshire_bull_terrier',\n",
    " 36: 'wheaten_terrier', 37: 'yorkshire_terrier'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def get_tensor_detected_image(sess, img_array, use_copied_array):\n",
    "    \n",
    "    rows = img_array.shape[0]\n",
    "    cols = img_array.shape[1]\n",
    "    if use_copied_array:\n",
    "        draw_img = img_array.copy()\n",
    "    else:\n",
    "        draw_img = img_array\n",
    "    \n",
    "    inp = img_array[:, :, [2, 1, 0]]  # BGR2RGB\n",
    "    \n",
    "    start = time.time()\n",
    "    # Object Detection 수행. \n",
    "    out = sess.run([sess.graph.get_tensor_by_name('num_detections:0'),\n",
    "                    sess.graph.get_tensor_by_name('detection_scores:0'),\n",
    "                    sess.graph.get_tensor_by_name('detection_boxes:0'),\n",
    "                    sess.graph.get_tensor_by_name('detection_classes:0')],\n",
    "                   feed_dict={'image_tensor:0': inp.reshape(1, inp.shape[0], inp.shape[1], 3)})\n",
    "    \n",
    "    green_color=(0, 255, 0)\n",
    "    red_color=(0, 0, 255)\n",
    "    \n",
    "    # Bounding Box 시각화 \n",
    "    # Detect된 Object 별로 bounding box 시각화 \n",
    "    num_detections = int(out[0][0])\n",
    "    for i in range(num_detections):\n",
    "        # class id와 object class score, bounding box정보를 추출\n",
    "        classId = int(out[3][0][i])\n",
    "        score = float(out[1][0][i])\n",
    "        bbox = [float(v) for v in out[2][0][i]]\n",
    "        if score > 0.4:\n",
    "            left = bbox[1] * cols\n",
    "            top = bbox[0] * rows\n",
    "            right = bbox[3] * cols\n",
    "            bottom = bbox[2] * rows\n",
    "            # cv2의 rectangle(), putText()로 bounding box의 클래스명 시각화 \n",
    "            cv2.rectangle(draw_img, (int(left), int(top)), (int(right), int(bottom)), green_color, thickness=2)\n",
    "            caption = \"{}: {:.4f}\".format(labels_to_names[classId], score)\n",
    "            cv2.putText(draw_img, caption, (int(left), int(top + 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.4, red_color, 1)\n",
    "    \n",
    "    print('Detection 수행시간:',round(time.time() - start, 3),\"초\")\n",
    "    return draw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#inference graph를 읽음. .\n",
    "with tf.gfile.FastGFile('./train_pet/training/frozen_inference_graph.pb', 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    # Session 시작하고 inference graph 모델 로딩 \n",
    "    sess.graph.as_default()\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "    \n",
    "    # 입력 이미지 생성, Object Detection된 image 반환, 반환된 image의 BGR을 RGB로 변경 \n",
    "    for image_file in ['Russian_Blue_29.jpg', 'yorkshire_terrier_176.jpg', 'german_shorthaired_127.jpg' ]:\n",
    "        img = cv2.imread(os.path.join(IMAGE_DIR, image_file))\n",
    "        draw_img = get_tensor_detected_image(sess, img, True)\n",
    "\n",
    "        img_rgb = cv2.cvtColor(draw_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenCV로 Object Detection 수행\n",
    "* tensorflow용 config파일을 opencv 용 Config 파일로 변경 필요. \n",
    "* frozen_inference_graph.pb는 그대로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/tf_text_graph_common.py\n",
    "# wget https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/tf_text_graph_ssd.py\n",
    "# python tf_text_graph_ssd.py --input training/frozen_inference_graph.pb --config config/pets_new.config --output config/graph.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "cv_net = cv2.dnn.readNetFromTensorflow('./train_pet/training/frozen_inference_graph.pb', \n",
    "                                     './train_pet/config/graph.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detected_img(cv_net, img_array, score_threshold, use_copied_array=True, is_print=True):\n",
    "    \n",
    "    rows = img_array.shape[0]\n",
    "    cols = img_array.shape[1]\n",
    "    \n",
    "    draw_img = None\n",
    "    if use_copied_array:\n",
    "        draw_img = img_array.copy()\n",
    "        #draw_img = cv2.cvtColor(draw_img, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        draw_img = img_array\n",
    "    \n",
    "    cv_net.setInput(cv2.dnn.blobFromImage(img_array, size=(300, 300), swapRB=True, crop=False))\n",
    "    \n",
    "    start = time.time()\n",
    "    cv_out = cv_net.forward()\n",
    "    \n",
    "    green_color=(0, 255, 0)\n",
    "    red_color=(0, 0, 255)\n",
    "\n",
    "    # detected 된 object들을 iteration 하면서 정보 추출\n",
    "    for detection in cv_out[0,0,:,:]:\n",
    "        score = float(detection[2])\n",
    "        class_id = int(detection[1])\n",
    "        # detected된 object들의 score가 0.3 이상만 추출\n",
    "        if score > score_threshold:\n",
    "            # detected된 object들은 image 크기가 (300, 300)으로 scale된 기준으로 예측되었으므로 다시 원본 이미지 비율로 계산\n",
    "            left = detection[3] * cols\n",
    "            top = detection[4] * rows\n",
    "            right = detection[5] * cols\n",
    "            bottom = detection[6] * rows\n",
    "            # labels_to_names 딕셔너리로 class_id값을 클래스명으로 변경. opencv에서는 class_id + 1로 매핑해야함.\n",
    "            caption = \"{}: {:.4f}\".format(labels_to_names[class_id], score)\n",
    "\n",
    "            #cv2.rectangle()은 인자로 들어온 draw_img에 사각형을 그림. 위치 인자는 반드시 정수형.\n",
    "            cv2.rectangle(draw_img, (int(left), int(top)), (int(right), int(bottom)), color=green_color, thickness=2)\n",
    "            cv2.putText(draw_img, caption, (int(left), int(top + 15)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, red_color, 2)\n",
    "    if is_print:\n",
    "        print('Detection 수행시간:',round(time.time() - start, 2),\"초\")\n",
    "\n",
    "    return draw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow inference 모델 로딩\n",
    "cv_net = cv2.dnn.readNetFromTensorflow('./train_pet/training/frozen_inference_graph.pb', \n",
    "                                     './train_pet/config/graph.pbtxt')\n",
    "for image_file in ['Russian_Blue_29.jpg', 'yorkshire_terrier_176.jpg', 'german_shorthaired_127.jpg' ]:\n",
    "    img = cv2.imread(os.path.join(IMAGE_DIR, image_file))\n",
    "    # Object Detetion 수행 후 시각화 \n",
    "    draw_img = get_detected_img(cv_net, img, score_threshold=0.4, use_copied_array=True, is_print=True)\n",
    "    img_rgb = cv2.cvtColor(draw_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_obj]",
   "language": "python",
   "name": "conda-env-tf_obj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
