{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Esri Challenge\n",
    "* 항공 사진 이미지를 기반으로 자동차와 수영장을 Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### annotation 디렉토리와 image 디렉토리 설정. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation과 image 디렉토리 설정. annotation디렉토리에 있는 파일 확인. \n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "HOME_DIR = str(Path.home())\n",
    "ANNO_DIR = os.path.join(HOME_DIR, 'DLCV/data/poolncar/training_data/training_data/labels')\n",
    "IMAGE_DIR = os.path.join(HOME_DIR, 'DLCV/data/poolncar/training_data/training_data/images')\n",
    "print(ANNO_DIR)\n",
    "\n",
    "files = os.listdir(ANNO_DIR)\n",
    "print('파일 개수는:',len(files))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML 형태의 annotation을 csv 형태로 변환\n",
    "* class 명은 1과 2\n",
    "* Object가 작아서 위치 좌표가 소수점까지 표시됨. pixel 단위는 정수형이므로 정수형으로 변환하되 ceil 적용하여 조금 이동하여 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ~/DLCV/data/poolncar/training_data/training_data/labels/000000040.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import math\n",
    "\n",
    "classes = ['1','2']\n",
    "\n",
    "# XML 파일을 Pandas DataFrame으로 변환 한뒤 DataFrame의 to_csv()를 이용하여 csv 파일로 생성하고 DataFrame반환\n",
    "def xml_to_csv(xml_files, output_filename):\n",
    "    xml_list = []\n",
    "    # xml 확장자를 가진 모든 파일의 절대 경로로 xml_file할당. \n",
    "    for xml_file in xml_files:\n",
    "        # xml 파일을 parsing하여 XML Element형태의 Element Tree를 생성하여 object 정보를 추출. \n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        if root.iter('object') is not None:\n",
    "            for obj in root.iter('object'):\n",
    "                full_image_name = os.path.join(IMAGE_DIR, root.find('filename').text)\n",
    "                cls = obj.find('name').text\n",
    "                if cls not in classes:\n",
    "                    continue\n",
    "\n",
    "                xmlbox = obj.find('bndbox')\n",
    "                # 위치 좌표가 소수점까지 표시됨. pixel 단위는 정수형이므로 변환하되 ceil로 조금 이동하여 변환\n",
    "                x1 = math.ceil(float(xmlbox.find('xmin').text))\n",
    "                y1 = math.ceil(float(xmlbox.find('ymin').text))\n",
    "                x2 = math.ceil(float(xmlbox.find('xmax').text))\n",
    "                y2 = math.ceil(float(xmlbox.find('ymax').text))\n",
    "                if x1 == x2 or y1 == y2:\n",
    "                    continue\n",
    "                value = (full_image_name, x1, y1, x2, y2, cls)\n",
    "\n",
    "                # object별 정보를 tuple형태로 xml_list에 저장. \n",
    "                xml_list.append(value)\n",
    "    # 모든 object별 정보를 DataFrame으로 생성하고 이를 CSV 파일로 생성하고 DataFrame은 반환. \n",
    "    column_name = ['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'class']\n",
    "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "    xml_df.to_csv(output_filename, index=None, header=None)\n",
    "    return xml_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train과 validation 용으로 파일 분리. \n",
    "* validation은 약 10% 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "all_xml_files = glob.glob(ANNO_DIR + '/*.xml')\n",
    "file_cnt = len(all_xml_files)\n",
    "valid_size = file_cnt//10\n",
    "valid_index = np.random.choice(file_cnt, valid_size)\n",
    "\n",
    "valid_files = [ all_xml_files[i] for i in valid_index ]\n",
    "train_files = [xml_file for xml_file in all_xml_files if xml_file not in valid_files ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV 파일로 전체/학습/검증 annotation 저장. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation 디렉토리 밑에 csv로 저장\n",
    "all_df = xml_to_csv(all_xml_files, os.path.join(ANNO_DIR, 'poolncar_anno.csv'))\n",
    "train_df = xml_to_csv(train_files, os.path.join(ANNO_DIR, 'poolncar_train_anno.csv'))\n",
    "valid_df = xml_to_csv(valid_files, os.path.join(ANNO_DIR, 'poolncar_valid_anno.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class name과 class id 매핑 파일을 ANNO_DIR에 poolncar_class.txt 로 생성\n",
    "* class name은 1(car)과 2(pool)로 되어 있음. id는 0 부터 시작해야 하므로\n",
    "* 1, 0  \n",
    "  2, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "test_image = cv2.imread(os.path.join(HOME_DIR,'DLCV/data/poolncar/training_data/training_data/images/000000040.jpg'))\n",
    "test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "print(test_image.shape)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esri 데이터 세트 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir, walk\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from os.path import join\n",
    "\n",
    "from keras_retinanet import models\n",
    "from keras_retinanet.bin.train import create_generators,create_models,create_callbacks\n",
    "from keras_retinanet.models import backbone,load_model,convert_model\n",
    "from keras_retinanet.utils.config import read_config_file,parse_anchor_parameters\n",
    "from keras_retinanet.utils.visualization import draw_boxes\n",
    "\n",
    "#from imgaug import augmenters as iaa\n",
    "\n",
    "tf.set_random_seed(31) # SEEDS MAKE RESULTS MORE REPRODUCABLE\n",
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### anchor  box 정보를 config.ini에 저장. \n",
    "* Object 들이 너무 작기 때문에 default anchor로 수행 성능이 저하될 수 있음. \n",
    "* anchor 최적화 스크립트 수행 후 anchor 값을 config.init에 설정할 수 있음. \n",
    "*  Improving RetinaNet for CT Lesion Detection with Dense Masks from Weak RECIST Labels에 사용된 anchor box 최적화 모듈을 https://github.com/martinzlocha/anchor-optimization/ 에서 다운로드 가능 \n",
    "* 여기서는 Winning 솔루션으로 설정된 anchor box를 그대로 사용함. \n",
    "\n",
    "#### Default anchor box 설정\n",
    "sizes   = 32 64 128 256 512\n",
    "strides = 8 16 32 64 128\n",
    "ratios  = 0.5 1 2 3\n",
    "scales  = 1 1.2 1.6\n",
    "\n",
    "#### Winning 솔루션으로 설정된 anchor box , 자동차의 경우 좀더 높이가 강조된 anchor box를 설정. \n",
    "sizes   = 32 64 128 256 512\n",
    "strides = 8 16 32 64 128\n",
    "ratios  = 0.25 0.5 0.75 1 1.5 2 4 6 8 10\n",
    "scales  = 0.5 1 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./keras-retinanet/snapshots/config_poolncar.ini','w') as f:\n",
    "    f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512\\nstrides = 8 16 32 64 128\\nratios  = 0.25 0.5 0.75 1 1.5 2 4 6 8 10\\nscales  = 0.5 1 2\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주요 환경 설정\n",
    "* 학습과 검증을 위한 csv annotation설정\n",
    "* backbone은 resnet50\n",
    "* batch_size=8\n",
    "* epochs=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = backbone('resnet50')\n",
    "files = os.listdir(IMAGE_DIR)\n",
    "\n",
    "class args:\n",
    "    batch_size = 8\n",
    "    config = read_config_file('./keras-retinanet/snapshots/config_poolncar.ini')\n",
    "    random_transform =True # Image augmentation\n",
    "    annotations = os.path.join(ANNO_DIR, 'poolncar_train_anno.csv')\n",
    "    #val_annotations = None\n",
    "    val_annotations = os.path.join(ANNO_DIR, 'poolncar_valid_anno.csv')\n",
    "    classes = os.path.join(ANNO_DIR, 'poolncar_class.txt')\n",
    "    # 기본값은 min_side=800, max_side=1333\n",
    "    image_min_side = 672\n",
    "    image_max_side = 672\n",
    "    no_resize=None\n",
    "    dataset_type = 'csv'\n",
    "    tensorboard_dir = ''\n",
    "    evaluation = False\n",
    "    snapshots = True\n",
    "    snapshot_path = './keras-retinanet/snapshots/poolncar'\n",
    "    backbone = 'resnet50'\n",
    "    epochs = 35\n",
    "    steps = len(files)//(batch_size)\n",
    "    weighted_average = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습용 DataGenerator, 검증용 DataGenerator생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen,valid_gen = create_generators(args,b.preprocess_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습과 예측 기반 모델 생성\n",
    "* Resnet50 backend 기반 모델 생성하고 이를 반환\n",
    "* 단일 GPU 모델에서는 model과 training_model이 서로 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, training_model, prediction_model = create_models(\n",
    "            backbone_retinanet=b.retinanet,\n",
    "            num_classes=train_gen.num_classes(),\n",
    "            weights=None,\n",
    "            multi_gpu=False,\n",
    "            freeze_backbone=True,\n",
    "            lr=1e-3,\n",
    "            config=args.config\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### callback 생성. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = create_callbacks(\n",
    "    model,\n",
    "    training_model,\n",
    "    prediction_model,\n",
    "    valid_gen,\n",
    "    args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training 모델에 최초 weight 로딩은 pretrained된 coco 모델의 weight값으로 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.load_weights('./keras-retinanet/snapshots/resnet50_coco_best_v2.1.0.h5',skip_mismatch=True,by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 수행\n",
    "* 주어진 epoch만큼, callback을 적용하며 training 모델의 학습 수행. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.fit_generator(generator=train_gen,\n",
    "        steps_per_epoch=args.steps,\n",
    "        epochs=args.epochs,\n",
    "        verbose=1,\n",
    "        validation_data=valid_gen, \n",
    "        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  convert_model.py를 이용하여 가장 마지막에 학습된 모델을 inference모델로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd ./keras-retinanet/snapshots; ls -lia\n",
    "# export poolncar_dir=~/DLCV/Detection/retina/keras-retinanet/snapshots\n",
    "#./keras_retinanet/bin/convert_model.py --config=$poolncar_dir/config_poolncar.ini $poolncar_dir/poolncar/resnet50_csv_35.h5 $poolncar_dir/poolncar/poolncar_inference.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('keras-retinanet','snapshots/poolncar/poolncar_inference.h5')\n",
    "print(model_path)\n",
    "# load retinanet model\n",
    "poolncar_retina_model = models.load_model(model_path, backbone_name='resnet50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 또는 API를 이용하여 가장 마지막에 학습된 모델을 inference모델로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir, walk\n",
    "from os.path import join\n",
    "from keras_retinanet.bin.train import create_models\n",
    "from keras_retinanet.models import backbone,convert_model\n",
    "from keras_retinanet.utils.config import read_config_file,parse_anchor_parameters\n",
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(31)\n",
    "np.random.seed(17)\n",
    "\n",
    "model, training_model, prediction_model = create_models(\n",
    "            backbone_retinanet=backbone('resnet50').retinanet,\n",
    "            num_classes=2,\n",
    "            weights=None,\n",
    "            multi_gpu=False,\n",
    "            freeze_backbone=False,\n",
    "            lr=1e-3,\n",
    "            config=read_config_file('./keras-retinanet/snapshots/config_poolncar.ini')\n",
    "        )\n",
    "\n",
    "training_model.load_weights('./keras-retinanet/snapshots/poolncar/resnet50_csv_35.h5')\n",
    "poolncar_retina_model = convert_model(training_model,anchor_params=parse_anchor_parameters(read_config_file('./keras-retinanet/snapshots/config_poolncar.ini')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference 모델을 이용하여 Object Detection 수행. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_retinanet.utils.image import read_image_bgr\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(cv2.cvtColor(read_image_bgr(os.path.join(IMAGE_DIR, '000000040.jpg')), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color\n",
    "\n",
    "def get_detected_image_retina(model, img_array, convert_RGB=True, is_print=True):\n",
    "    \n",
    "    # copy to draw on\n",
    "    draw = img_array.copy()\n",
    "    if convert_RGB:\n",
    "        draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "    img_array = preprocess_image(img_array)\n",
    "    # 학습시 사용된 image resize를 적용. \n",
    "    img_array, scale = resize_image(img_array, 672, 672)\n",
    "    \n",
    "    # process image\n",
    "    start = time.time()\n",
    "    boxes, scores, labels = model.predict_on_batch(np.expand_dims(img_array, axis=0))\n",
    "    if is_print:\n",
    "        print(\"object detection 처리 시간: \", round(time.time() - start,5))\n",
    "    \n",
    "    # correct for image scale\n",
    "    boxes /= scale\n",
    "    \n",
    "    classes=['1','2']\n",
    "\n",
    "    # visualize detections\n",
    "    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "        \n",
    "        # scores are sorted so we can break\n",
    "        print(score)\n",
    "        if score < 0.5:\n",
    "            break\n",
    "\n",
    "        color = label_color(label)\n",
    "\n",
    "        b = box.astype(int)\n",
    "\n",
    "        caption = \"{} {:.3f}\".format(classes[label], score)\n",
    "        print('caption:', caption)\n",
    "        cv2.rectangle(draw, (box[0],box[1]), (box[2], box[3]), color, thickness=2)\n",
    "        cv2.putText(draw, caption, (b[0], b[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "    \n",
    "    if is_print:\n",
    "        print(\"이미지 processing 시간2: \", round(time.time() - start,5))\n",
    "    \n",
    "    return draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array  = cv2.imread(os.path.join(IMAGE_DIR, '000000040.jpg'))\n",
    "detected_image = get_detected_image_retina(poolncar_retina_model,img_array, convert_RGB=True, is_print=True)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.axis('off')\n",
    "plt.imshow(detected_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation dataset 기반으로 mAP 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class args:\n",
    "    batch_size=8\n",
    "    dataset_type='csv'\n",
    "    score_threshold=0.05\n",
    "    iou_threshold=0.3\n",
    "    max_detections=100\n",
    "    image_min_side=672\n",
    "    image_max_side=672\n",
    "    config=None\n",
    "    annotations=os.path.join(ANNO_DIR, 'poolncar_valid_anno.csv')\n",
    "    classes=os.path.join(ANNO_DIR, 'poolncar_class.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_retinanet.bin.evaluate import create_generator as eval_create_generator\n",
    "from keras_retinanet.utils.eval import evaluate\n",
    "\n",
    "generator = eval_create_generator(args)\n",
    "\n",
    "average_precisions = evaluate(\n",
    "            generator,\n",
    "            poolncar_retina_model,\n",
    "            iou_threshold=args.iou_threshold,\n",
    "            score_threshold=args.score_threshold,\n",
    "            max_detections=args.max_detections,\n",
    "            save_path=None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluation\n",
    "total_instances = []\n",
    "precisions = []\n",
    "for label, (average_precision, num_annotations) in average_precisions.items():\n",
    "    print('{:.0f} instances of class'.format(num_annotations),\n",
    "          generator.label_to_name(label), 'with average precision: {:.4f}'.format(average_precision))\n",
    "    total_instances.append(num_annotations)\n",
    "    precisions.append(average_precision)\n",
    "\n",
    "if sum(total_instances) == 0:\n",
    "    print('No test instances found.')\n",
    "\n",
    "#print('Inference time for {:.0f} images: {:.4f}'.format(generator.size(), inference_time))\n",
    "\n",
    "print('mAP using the weighted average of precisions among classes: {:.4f}'.format(sum([a * b for a, b in zip(total_instances, precisions)]) / sum(total_instances)))\n",
    "print('mAP: {:.4f}'.format(sum(precisions) / sum(x > 0 for x in total_instances)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf115] *",
   "language": "python",
   "name": "conda-env-tf115-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
